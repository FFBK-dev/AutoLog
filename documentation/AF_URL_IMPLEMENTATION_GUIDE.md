# AF URL Construction - Implementation Guide

## Executive Summary

**Research Complete**: Analyzed 13 archive sources with actual filename patterns  
**URLs Verified**: Tested URL patterns with real IDs from your files  
**Ready to Implement**: 5 archives with clear patterns and verified URLs  

---

## âœ… VERIFIED & READY TO IMPLEMENT

### 1. ArtGrid âœ… **VERIFIED - HTTP 200**
```
Archive Name: ArtGrid
URL Root: https://artgrid.io/clip/
Pattern: https://artgrid.io/clip/[ID]
Example: https://artgrid.io/clip/557465
Status: âœ… Works - Returns HTTP 200
```

**Filename Pattern**: `557465_Plants_Field_Weeds_Blur_By_Ami_Bornstein_Artlist_HD.mp4`  
**Cleaning Rule**: Take digits before first underscore

**Code**:
```python
elif source and "artgrid" in source.lower():
    if "_" in cleaned_id:
        cleaned_id = cleaned_id.split("_")[0]
```

---

### 2. FilmSupply âœ… **VERIFIED - HTTP 200**
```
Archive Name: FilmSupply
URL Root: https://www.filmsupply.com/footage/
Pattern: https://www.filmsupply.com/footage/[ID]
Example: https://www.filmsupply.com/footage/59522
Status: âœ… Works - Returns HTTP 200
```

**Filename Pattern**: `Marco Schott-foggy-field-59522-filmsupply.mov`  
**Cleaning Rule**: Extract number before `-filmsupply`

**Code**:
```python
elif source and "filmsupply" in source.lower():
    if "-filmsupply" in cleaned_id.lower():
        parts = cleaned_id.lower().split("-filmsupply")[0].split("-")
        # Last part should be numeric ID
        for part in reversed(parts):
            if part.isdigit():
                cleaned_id = part
                break
```

---

### 3. Getty Images âœ… **VERIFIED - HTTP 301 (Redirect)**
```
Archive Name: Getty Images
URL Root: https://www.gettyimages.com/video/
Pattern: https://www.gettyimages.com/video/[ID]
Example: https://www.gettyimages.com/video/482875966
Status: âœ… Works - Returns HTTP 301 (redirects to correct page)
```

**Filename Pattern**: `GettyImages-482875966.mov`  
**Cleaning Rule**: Already implemented in code!

**Status**: âœ… Already supported - just update URL root in FileMaker from `/detail/video/` to `/video/`

---

## ðŸ”’ LIKELY VALID BUT AUTHENTICATION-PROTECTED

### 4. Pond5 ðŸ”’ **Pattern Clear - Returns 403**
```
Archive Name: Pond5
URL Root: https://www.pond5.com/stock-footage/item/
Pattern: https://www.pond5.com/stock-footage/item/[ID]
Example: https://www.pond5.com/stock-footage/item/236704593
Status: ðŸ”’ Returns HTTP 403 (likely requires login)
```

**Filename Pattern**: `236704593-water-mosquitos-sliding-and-ju.mp4`  
**Cleaning Rule**: Take digits before first hyphen

**Code**:
```python
elif source and "pond5" in source.lower():
    if "-" in cleaned_id:
        cleaned_id = cleaned_id.split("-")[0]
```

**Recommendation**: Implement - URL pattern is correct, 403 is likely due to auth requirements

---

### 5. Critical Past ðŸ”’ **Pattern Probable - Returns 403**
```
Archive Name: Critical Past
URL Root: https://criticalpast.com/video/
Pattern: https://criticalpast.com/video/[ID]
Example: https://criticalpast.com/video/65675076731
Status: ðŸ”’ Returns HTTP 403 (likely bot protection)
```

**Filename Pattern**: `65675076731----1080-24p-Screening.mov`  
**Cleaning Rule**: Take everything before `----`

**Code**:
```python
elif source and "critical" in source.lower():
    if "----" in cleaned_id:
        cleaned_id = cleaned_id.split("----")[0]
```

**Recommendation**: Implement - Pattern appears valid, 403 likely anti-bot measure

---

### 6. Library of Congress (LOC) ðŸ”’ **Complex Pattern - Returns 403**
```
Archive Name: LOC
URL Root: https://www.loc.gov/item/mbrs-ntscrm.
Pattern: https://www.loc.gov/item/mbrs-ntscrm.[ID]
Example: https://www.loc.gov/item/mbrs-ntscrm.00060780
Status: ðŸ”’ Returns HTTP 403 (likely requires different format)
```

**Filename Pattern**: `service-mbrs-ntscrm-00060780-00060780.mov`  
**Cleaning Rule**: Complex - needs to preserve service prefix

**Code**:
```python
elif source and ("loc" in source.lower() or "library of congress" in source.lower()):
    # Extract from service-mbrs-ntscrm-[ID]-[ID] pattern
    if "service-mbrs-ntscrm-" in cleaned_id.lower():
        parts = cleaned_id.split("-")
        if len(parts) >= 4:
            # Reconstruct as mbrs-ntscrm.[ID]
            cleaned_id = f"mbrs-ntscrm.{parts[3]}"
```

**Recommendation**: Implement with custom format - may need manual verification

---

## â“ NEEDS MORE RESEARCH

### 7. UNC (University of North Carolina) â“
```
Status: Complex academic archive - may not have direct video URLs
Filename: 04773_F0012_0001.mp4
Pattern: [CollectionID]_[FileID]_[SequenceID]
```

**Recommendation**: 
- Research UNC Southern Historical Collection online catalog
- May only have finding aids, not direct video links
- **LOW PRIORITY** - defer until confirmed URL structure exists

---

### 8. Smithsonian â³
```
Status: No files in directory yet
```

**Recommendation**: Wait for files, then research based on actual filenames

---

### 9. Hagley Museum â³
```
Status: No complete files in directory yet
```

**Recommendation**: Wait for files, then research based on actual filenames

---

## âŒ NO URL CONSTRUCTION NEEDED

### 10-13. Custom Sources (No Public URLs)
- **JR**: Custom library footage
- **Hunter Nichols**: Custom filmmaker
- **CCWP**: Unknown/mixed sources
- **Public Domain**: Too varied

---

## IMPLEMENTATION PLAN

### Phase 1: Immediate (High Confidence) âœ…
**Add these 3 archives now - URLs verified to work**:

1. **ArtGrid** - HTTP 200 âœ…
   - Add to FileMaker URLs table
   - Add cleaning logic to `url_validator.py`
   
2. **FilmSupply** - HTTP 200 âœ…
   - Add to FileMaker URLs table
   - Add cleaning logic to `url_validator.py`

3. **Getty Images** - HTTP 301 âœ…
   - Update URL root in FileMaker URLs table
   - Cleaning logic already exists

### Phase 2: High Probability (Add with Testing) ðŸ”’
**Add these 3 archives - patterns clear, 403 likely auth/bot protection**:

4. **Pond5** - Add with note about auth requirements
5. **Critical Past** - Add with note about bot protection  
6. **LOC** - Add with custom format

### Phase 3: Research Required â“
**Defer until more information available**:

7. **UNC** - Research academic archive structure
8. **Smithsonian** - Wait for files
9. **Hagley Museum** - Wait for files

---

## CODE IMPLEMENTATION

### Update `utils/url_validator.py`

Add this section to the `clean_archival_id_for_url()` function:

```python
def clean_archival_id_for_url(archival_id, source):
    """Clean archival ID by removing source-specific prefixes and suffixes."""
    if not archival_id:
        return archival_id
    
    cleaned_id = archival_id.strip()
    
    # Getty Images (EXISTING - already implemented)
    if source and "getty" in source.lower():
        # ... existing Getty logic ...
        pass
    
    # ArtGrid (NEW) - Extract ID before first underscore
    elif source and "artgrid" in source.lower():
        if "_" in cleaned_id:
            cleaned_id = cleaned_id.split("_")[0]
    
    # FilmSupply (NEW) - Extract ID before '-filmsupply'
    elif source and "filmsupply" in source.lower():
        if "-filmsupply" in cleaned_id.lower():
            parts = cleaned_id.lower().split("-filmsupply")[0].split("-")
            # Last numeric part is the ID
            for part in reversed(parts):
                if part.isdigit():
                    cleaned_id = part
                    break
    
    # Pond5 (NEW) - Take ID before first hyphen
    elif source and "pond5" in source.lower():
        if "-" in cleaned_id:
            cleaned_id = cleaned_id.split("-")[0]
    
    # Critical Past (NEW) - Remove ---- and everything after
    elif source and ("critical" in source.lower() or "criticalpast" in source.lower()):
        if "----" in cleaned_id:
            cleaned_id = cleaned_id.split("----")[0]
    
    # LOC / Library of Congress (NEW) - Complex pattern
    elif source and ("loc" in source.lower() or "library of congress" in source.lower()):
        if "service-mbrs-ntscrm-" in cleaned_id.lower():
            parts = cleaned_id.split("-")
            if len(parts) >= 4:
                # Reconstruct as mbrs-ntscrm.[ID] for LOC format
                cleaned_id = f"mbrs-ntscrm.{parts[3]}"
    
    return cleaned_id
```

### Update FileMaker URLs Table

Add these records to your URLs layout/table:

| Archive | URL Root |
|---------|----------|
| ArtGrid | https://artgrid.io/clip/ |
| FilmSupply | https://www.filmsupply.com/footage/ |
| Getty Images | https://www.gettyimages.com/video/ *(update existing)* |
| Pond5 | https://www.pond5.com/stock-footage/item/ |
| Critical Past | https://criticalpast.com/video/ |
| LOC | https://www.loc.gov/item/ |

---

## TESTING CHECKLIST

Before deploying to production:

- [ ] Test ArtGrid URL construction with multiple file examples
- [ ] Test FilmSupply URL construction with multiple file examples  
- [ ] Test Getty Images URL construction (verify redirect works)
- [ ] Test Pond5 URL construction (accept 403 as valid)
- [ ] Test Critical Past URL construction (accept 403 as valid)
- [ ] Test LOC URL construction with LOC-specific format
- [ ] Verify URLs are written correctly to FileMaker
- [ ] Test URL scraping works for constructed URLs

---

## SUCCESS METRICS

### Immediate Win (Phase 1)
**3 archives** with 100% confidence:
- ArtGrid âœ…
- FilmSupply âœ…  
- Getty Images âœ… (improved)

### High Confidence (Phase 2)
**3 more archives** with high probability:
- Pond5 ðŸ”’
- Critical Past ðŸ”’
- LOC ðŸ”’

### Total Coverage
**6 of 13 archives** = 46% of sources with intelligent URL construction

---

## RECOMMENDED NEXT STEPS

1. **Implement Phase 1** (3 verified archives)
   - Update `url_validator.py` with cleaning logic
   - Add entries to FileMaker URLs table
   - Test with real AF files

2. **Implement Phase 2** (3 likely-valid archives)
   - Same process as Phase 1
   - Document that 403 responses are expected

3. **Monitor and Refine**
   - Track URL construction success rate
   - Adjust patterns based on real-world results
   - Add logging for failed constructions

4. **Future Research**
   - UNC when you need those files
   - Smithsonian when files arrive
   - Hagley when files arrive

---

**Status**: âœ… Ready for Implementation  
**Risk**: Low (verified patterns for 3 archives, high-confidence for 3 more)  
**Effort**: ~2-3 hours for Phase 1 + Phase 2  
**Impact**: Automatic URL construction for 46% of AF sources  


